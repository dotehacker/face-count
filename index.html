<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live Face Detection</title>
    <link rel="icon" href="data:,">
</head>
<body>
    <h2>Live Face Detection</h2>
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>
    <p id="faceCount">Total Faces Detected: 0</p>
    <p id="fps">FPS: 0</p>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-detection"></script>

    <script>
        let detector;
        let video;
        let canvas;
        let ctx;
        let startTime;
        let frameCount = 0;
        let fpsDisplay = document.getElementById('fps');

        async function startFaceDetection() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                alert("Error: Camera access is not supported in this browser.");
                return;
            }

            try {
                // Configure detector to handle multiple faces
                detector = await faceDetection.createDetector(
                    faceDetection.SupportedModels.MediaPipeFaceDetector,
                    { 
                        runtime: 'tfjs',
                        maxFaces: 10,  // Allow up to 10 faces detection
                        modelType: 'short'  // Better performance, can use 'full' for accuracy
                    }
                );

                video = document.getElementById("video");
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                canvas = document.getElementById("canvas");
                ctx = canvas.getContext("2d");

                video.addEventListener('loadedmetadata', () => {
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                    startTime = performance.now();
                    detectFaces();
                });

            } catch (error) {
                console.error("Face Detection Error:", error);
                alert("Error: Unable to start face detection.");
            }
        }

        async function detectFaces() {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            const faces = await detector.estimateFaces(video);

            // Update face count display with current frame's detection
            document.getElementById("faceCount").innerText = `Faces Detected: ${faces.length}`;

            // Draw bounding boxes for visualization
            ctx.strokeStyle = "red";
            ctx.lineWidth = 20;
            faces.forEach(face => {
                const { xMin, yMin, width, height } = face.box;
                ctx.strokeRect(xMin, yMin, width, height);
            });

            // Calculate FPS
            frameCount++;
            const currentTime = performance.now();
            const elapsedTime = currentTime - startTime;
            const currentFps = frameCount / (elapsedTime / 1000);
            fpsDisplay.textContent = `FPS: ${currentFps.toFixed(2)}`;

            requestAnimationFrame(detectFaces);
        }

        startFaceDetection();
    </script>
</body>
</html>